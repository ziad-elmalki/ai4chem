{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Script for training models and selecting the best performing ones \n",
    "'''\n",
    "\n",
    "# basics\n",
    "import os\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# pandas is used to read/process data\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "from drfp import DrfpEncoder\n",
    "#For confusion Matrix \n",
    "import seaborn as sns\n",
    "\n",
    "# IMPORTS FOR FINGERPRINTS\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rxnrule.models.utils import load_data, smiles_to_mol, split_reactant, create_fingerprints \n",
    "from tqdm import tqdm\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# linear model\n",
    "from sklearn.linear_model import Lasso, LinearRegression, LogisticRegression, Ridge, RidgeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    max_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "# machine learning dependencies\n",
    "# train/test split\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Save the models \n",
    "import pickle \n",
    "\n",
    "\n",
    "\n",
    "# WRITE FUNCTIONS\n",
    "\n",
    "#ADD preprocess function that takes the path of dataframe and returns the train and test sets already splitted \n",
    "\n",
    "def preprocess(df_path):\n",
    "    \"\"\"Preprocessing function for the dataset.\"\"\"\n",
    "    X_data, y_data = load_data(df_path)\n",
    "    #X_data= X_data_full.sample(2000, random_state=32)\n",
    "    #y_data = y_data_full.sample(2000, random_state=32)\n",
    "    split_fp_data, merged_fp_data, Drfp_data = create_fingerprints(X_data)\n",
    "\n",
    "    # Dictionary to store split data for each fingerprint type\n",
    "    split_data_dict = {}\n",
    "\n",
    "    # Split the data we have into training and temporary data (80% train, 20% test) for each fingerprint\n",
    "    for fingerprint_type, X_FGs in [\n",
    "        (\"Morgan split Fp\", split_fp_data),\n",
    "        (\"Morgan Merged Fp\", merged_fp_data),\n",
    "        (\"DrFp\", Drfp_data),\n",
    "    ]:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_FGs, y_data, test_size=0.2, random_state=420\n",
    "        )\n",
    "\n",
    "        # Store split data in the dictionary\n",
    "        split_data_dict[fingerprint_type] = {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "\n",
    "    return split_data_dict\n",
    "\n",
    "\n",
    "def train_models(X_train, y_train, penalty_range, C_range, fit_intercept_range, n_estimators_range,max_depth_range, kernels ,save_path_prefix, results_file_path):\n",
    "    \"\"\"Defines and trains a random forest, a Support Vector Machine, and a Logistic Regression model.\n",
    "    Performs grid search and saves the best model for each type using pickle.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Contains the Reactant SMILES data we wish to train our models on.\n",
    "        y_train (pd.DataFrame): Contains the Labels of the Reactants Compatibility (0 for compatible reactants, 1 for incompatible reactants).\n",
    "        penalty_range (list): List of penalties to search through.\n",
    "        C_range (list): List of C values to search through.\n",
    "        fit_intercept_range (list): List of boolean values for fit_intercept.\n",
    "        save_path_prefix (str): Prefix for the path to save the best models.\n",
    "        split_data_dict (dict): Dictionary containing split data for each fingerprint type.\n",
    "\n",
    "    Returns:\n",
    "        best_models: Dictionary containing the best model for each type.\n",
    "    \"\"\"\n",
    "    rand_for = RandomForestClassifier(random_state=33)\n",
    "    log_reg = LogisticRegression(max_iter=500)\n",
    "    svc = SVC(random_state=33)\n",
    "\n",
    "    models = [rand_for, log_reg, svc]\n",
    "    best_models = {}\n",
    "\n",
    "    for model_type, model in zip(['Random Forest', 'Logistic Regression', 'Support Vector'], models):\n",
    "        best_accuracy = 0.0\n",
    "        best_model = None\n",
    "        best_fingerprint_type = None\n",
    "            \n",
    "        if model_type=='Logistic Regression':\n",
    "            param_grid = {\n",
    "                'penalty': penalty_range,\n",
    "                'C': C_range,\n",
    "                'fit_intercept': fit_intercept_range\n",
    "            }\n",
    "\n",
    "        if model_type=='Random Forest': \n",
    "            param_grid = {\n",
    "                'n_estimators':n_estimators_range,\n",
    "                'max_depth' : max_depth_range\n",
    "            }\n",
    "        \n",
    "        if model_type=='Support Vector': \n",
    "            param_grid = {\n",
    "                'C': C_range,\n",
    "                'kernel' : kernels\n",
    "        \n",
    "            }\n",
    "\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        accuracy = grid_search.best_score_\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "        best_models[f'{model_type}'] = best_model\n",
    "        print(f\"Best parameters for {model_type}: {grid_search.best_params_}\")\n",
    "\n",
    "        # Save the best model using pickle\n",
    "        save_path = f\"{save_path_prefix}_{model_type.replace(' ', '_')}.pkl\"\n",
    "        with open(save_path, 'wb') as model_file:\n",
    "            pickle.dump(best_model, model_file)\n",
    "\n",
    "        with open(results_file_path, \"a\") as results_file: \n",
    "            # Append results to the text file\n",
    "            results_file.write(f\"Finger Print: {fp}\\n\")\n",
    "            results_file.write(f\"Model Type: {model_type}\\n\")\n",
    "            results_file.write(f\"Best Parameters: {grid_search.best_params_}\\n\")\n",
    "            results_file.write(f\"Best Accuracy: {best_accuracy}\\n\")\n",
    "            results_file.write('\\n')\n",
    "\n",
    "    return best_models\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, classes):\n",
    "    cm = confusion_matrix(y_pred, y_test) \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=classes, yticklabels=classes) \n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.savefig(\"Confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_model(models_dict, X_test, y_test,results_file_path): \n",
    "    \"\"\"Evaluate multiple models and return the best one based on accuracy.\n",
    "\n",
    "    Args:\n",
    "        models_dict (dict): Dictionary containing models for different types.\n",
    "        X_test, y_test (pd.DataFrame): The data to test the models.\n",
    "        fingerprint_type (str): The type of fingerprint used in training.\n",
    "\n",
    "    Returns:\n",
    "        best_model: The best model based on accuracy.\n",
    "        best_metrics: Dictionary containing metrics of the best model.\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "    best_metrics = {}\n",
    "    fingerprint = None\n",
    "    classes = ['Compatible', 'Incompatible']\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"Evaluating model: {model_name}\")\n",
    " \n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        max_err = max_error(y_test, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\")\n",
    "        print(f\"MSE: {mse}, Max Error: {max_err}\")\n",
    "\n",
    "        plot_confusion_matrix(y_test, y_pred, classes)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "            fingerprint = fp \n",
    "            best_metrics = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'mse': mse,\n",
    "                'max_err': max_err\n",
    "            }\n",
    "\n",
    "    print(f\"The best model is: {type(best_model).__name__}\")\n",
    "    print(f\"with accuracy: {best_metrics['accuracy']} and fingerprint {fingerprint}\")\n",
    "\n",
    "    with open(results_file_path, \"a\") as results_file:\n",
    "        results_file.write(\"This is the testing results\\n\")\n",
    "        results_file.write(f\"The best model is: {type(best_model).__name__}\\n\") \n",
    "        results_file.write(f\"with test accuracy: {best_metrics} and fingerprint {fingerprint}\\n\") \n",
    "        results_file.write('\\n')\n",
    "\n",
    "    return best_model, best_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # LOAD DATA\n",
    "    file_path = \"data/processed/generated_data.csv\" #/Users/ziadelmalki/Desktop/rxnrule/data/processed/generated_data.csv\n",
    "    split_data_dict= preprocess(file_path)\n",
    "\n",
    "    # Set ranges for grid search\n",
    "    penalty_range = ['l1', 'l2']\n",
    "    C_range = [0.001, 0.01, 0.1, 1, 10]\n",
    "    fit_intercept_range = [True, False]\n",
    "    kernels= [\"linear\", \"poly\", \"rbf\"]\n",
    "    n_estimators_range = [10,100,200]\n",
    "    max_depth_range = [3,4,5]\n",
    "    fingerprint_types = [\"Morgan split Fp\",\"Morgan Merged Fp\",\"DrFp\"]\n",
    "    final_models = {}\n",
    "\n",
    "    for fp in tqdm(fingerprint_types):\n",
    "        \n",
    "        print(f\"For the FngerPrint: {fp}:\")\n",
    "        X_train = split_data_dict[fp]['X_train']\n",
    "        y_train = split_data_dict[fp]['y_train']\n",
    "        results_file_path = 'FullSet_models_training_results.txt'\n",
    "\n",
    "        #We get the best models based on Grid search, one per classification type \n",
    "        best_models = train_models(X_train, y_train , penalty_range, C_range, fit_intercept_range, n_estimators_range, max_depth_range, kernels, fp,results_file_path)\n",
    "        print(best_models)\n",
    "\n",
    "        X_test = split_data_dict[fp]['X_test']\n",
    "        y_test = split_data_dict[fp]['y_test']\n",
    "\n",
    "        #Out of the three we pick the best per Finger print\n",
    "        best_model, best_metrics = evaluate_model(best_models,X_test, y_test, results_file_path)\n",
    "        print(f\"The best model is {best_model} with best metrics:{best_metrics}\")\n",
    "\n",
    "        #We store the best one per fingerprint \n",
    "        \n",
    "        final_models[f\"{fp}\"] = (best_model, best_metrics) \n",
    "        print(f'The final three models are: {final_models}')\n",
    "\n",
    "    with open(results_file_path, \"a\") as results_file:\n",
    "        results_file.write(f\"The best 3 models are: \\n\")\n",
    "        for fingerprint, (model, metrics) in final_models.items():\n",
    "            results_file.write(f\"{model} with {fingerprint} and Results: {metrics}\\n\") \n",
    "        results_file.write('\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('../../../../docs/data/cycpeptdb_clean_onlyPC.csv')\n",
    "\n",
    "np.random.seed(42)\n",
    "X = df.drop['Permeability']#features\n",
    "y = df['Permeability']#labels\n",
    "\n",
    "# Step 3: Perform PCA to reduce dimensions to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train a linear regression model with a bias term\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred_train = regressor.predict(X_train)\n",
    "y_pred_test = regressor.predict(X_test)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "\n",
    "print(f\"Regression coefficients: {regressor.coef_}\")\n",
    "print(f\"Bias term: {regressor.intercept_}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
